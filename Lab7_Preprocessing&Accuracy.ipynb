{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7: Data Preprocessing and Accuracy Measures\n",
    "This tutorial uses standard python libraries used for data processing. The lab also introduces some basic accuracy measures used for performance evaluation purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, scipy, numpy as np\n",
    "import sklearn.preprocessing\n",
    "import sklearn.impute\n",
    "\n",
    "ds = pd.read_excel(\"Job_Scheduling.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "Data scientists come across many datasets and not all of them may be well formatted or noise free. While doing any kind of analysis with data it is important to clean it, as raw data can be highly unstructured with noise or missing data or data that is varying in scales which makes it hard to extract useful information. Pre-processing refers to the transformations applied to our data before feeding it to any machine learning algorithm. It is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Job Id  Burst time  Arrival Time  Preemptive  Resources\n",
      "0      334       179.0        0.6875         1.0        4.0\n",
      "1      234       340.0        0.7800         0.0        4.0\n",
      "2      138       143.0        0.9150         1.0        4.0\n",
      "3      463       264.0           NaN         0.0        5.0\n",
      "4      283       216.0        0.5550         0.0        6.0\n",
      "5       88        36.0        0.6625         0.0        5.0\n",
      "6      396       128.0        0.1975         1.0        NaN\n",
      "7      470       203.0        0.9875         1.0        4.0\n",
      "8      335       271.0        0.0275         0.0        3.0\n",
      "9      272       399.0        0.2150         NaN        3.0\n",
      "10     237         NaN        0.4825         1.0        4.0\n",
      "11     318       311.0        0.5675         1.0        1.0\n",
      "12      84       111.0        0.2725         1.0        2.0\n",
      "13     311        87.0           NaN         0.0        7.0\n",
      "14     163       103.0        0.4600         0.0        5.0\n",
      "15     453       213.0        0.0775         1.0        1.0\n",
      "16     176       251.0        0.7050         0.0        6.0\n",
      "17     449        49.0        0.2550         1.0        4.0\n",
      "18      11       168.0        0.3175         1.0        7.0\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementing the Transform\n",
    "\n",
    "#### fit() : \n",
    "used for generating learning model parameters from training data\n",
    "\n",
    "#### transform() : \n",
    "parameters generated from fit() method,applied upon model to generate transformed data set.\n",
    "\n",
    "#### fit_transform() : \n",
    "combination of fit() and transform() api on same data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.340e+02 1.790e+02 6.875e-01 1.000e+00]\n",
      " [2.340e+02 3.400e+02 7.800e-01 0.000e+00]\n",
      " [1.380e+02 1.430e+02 9.150e-01 1.000e+00]\n",
      " [4.630e+02 2.640e+02       nan 0.000e+00]\n",
      " [2.830e+02 2.160e+02 5.550e-01 0.000e+00]\n",
      " [8.800e+01 3.600e+01 6.625e-01 0.000e+00]\n",
      " [3.960e+02 1.280e+02 1.975e-01 1.000e+00]\n",
      " [4.700e+02 2.030e+02 9.875e-01 1.000e+00]\n",
      " [3.350e+02 2.710e+02 2.750e-02 0.000e+00]\n",
      " [2.720e+02 3.990e+02 2.150e-01       nan]\n",
      " [2.370e+02       nan 4.825e-01 1.000e+00]\n",
      " [3.180e+02 3.110e+02 5.675e-01 1.000e+00]\n",
      " [8.400e+01 1.110e+02 2.725e-01 1.000e+00]\n",
      " [3.110e+02 8.700e+01       nan 0.000e+00]\n",
      " [1.630e+02 1.030e+02 4.600e-01 0.000e+00]\n",
      " [4.530e+02 2.130e+02 7.750e-02 1.000e+00]\n",
      " [1.760e+02 2.510e+02 7.050e-01 0.000e+00]\n",
      " [4.490e+02 4.900e+01 2.550e-01 1.000e+00]\n",
      " [1.100e+01 1.680e+02 3.175e-01 1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x = ds.iloc[:,0:4].values #Extracting all the entries from column 0-4 from the 2D array\n",
    "np.set_printoptions(precision=3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas iloc function\n",
    "iloc returns a Pandas Series when one row is selected, and a Pandas DataFrame when multiple rows are selected, or if any column in full is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  4.  4.  5.  6.  5. nan  4.  3.  3.  4.  1.  2.  7.  5.  1.  6.  4.\n",
      "  7.]\n"
     ]
    }
   ],
   "source": [
    "y=ds.iloc[:,4].values     #Extracting all entries from column 4 from the 2D array\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values\n",
    "Machine learning models cannot accommodate missing fields in the data they are provided with. So, the missing fields must be filled with values that will not affect the variance of the data and make it less noisy. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n",
    "\n",
    "Note: More details on the vast majority of Imputers available for different data types can be accessed on https://scikit-learn.org/stable/modules/impute.html#impute\n",
    "\n",
    "The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. \n",
    "\n",
    "We have some missing fields in the data denoted by “nan” which is an acronym for “not a number”. \n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0) that contain the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.340e+02 1.790e+02 6.875e-01 1.000e+00]\n",
      " [2.340e+02 3.400e+02 7.800e-01 0.000e+00]\n",
      " [1.380e+02 1.430e+02 9.150e-01 1.000e+00]\n",
      " [4.630e+02 2.640e+02 4.803e-01 0.000e+00]\n",
      " [2.830e+02 2.160e+02 5.550e-01 0.000e+00]\n",
      " [8.800e+01 3.600e+01 6.625e-01 0.000e+00]\n",
      " [3.960e+02 1.280e+02 1.975e-01 1.000e+00]\n",
      " [4.700e+02 2.030e+02 9.875e-01 1.000e+00]\n",
      " [3.350e+02 2.710e+02 2.750e-02 0.000e+00]\n",
      " [2.720e+02 3.990e+02 2.150e-01 5.556e-01]\n",
      " [2.370e+02 1.929e+02 4.825e-01 1.000e+00]\n",
      " [3.180e+02 3.110e+02 5.675e-01 1.000e+00]\n",
      " [8.400e+01 1.110e+02 2.725e-01 1.000e+00]\n",
      " [3.110e+02 8.700e+01 4.803e-01 0.000e+00]\n",
      " [1.630e+02 1.030e+02 4.600e-01 0.000e+00]\n",
      " [4.530e+02 2.130e+02 7.750e-02 1.000e+00]\n",
      " [1.760e+02 2.510e+02 7.050e-01 0.000e+00]\n",
      " [4.490e+02 4.900e+01 2.550e-01 1.000e+00]\n",
      " [1.100e+01 1.680e+02 3.175e-01 1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Filling missing Values\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X = imp.fit_transform(x)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping your class attribute\n",
    "\n",
    "A numpy imputer expects a 2D array as input. In order to impute your class attribute, you need to convert it into a 2D array first. This can be done by reshaping your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.    4.    4.    5.    6.    5.    4.167 4.    3.    3.    4.    1.\n",
      " 2.    7.    5.    1.    6.    4.    7.   ]\n"
     ]
    }
   ],
   "source": [
    "# As highlighted earlier y holds the shape of a 1D array, where as imputation transforms demand a 2D array, so reshaping y\n",
    "Y = y.reshape(-1,1) #reshaping the numpy array with -1 parameter (numpy is to figure out the dimension)\n",
    "Y = imp.fit_transform(Y)\n",
    "Y = Y.reshape(-1)   # putting y bank in its original shape\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the 'most_frequent' or 'constant' strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'x']\n",
      " ['a' 'y']\n",
      " ['a' 'y']\n",
      " ['b' 'y']]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                   [np.nan, \"y\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype=\"category\")\n",
    "\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "print(imp.fit_transform(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean feature value of the two nearest neighbors of samples with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "nan = np.nan\n",
    "P = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imputer.fit_transform(P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling the Variables\n",
    "When the data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors. Data can be rescaled using scikit-learn using the MinMaxScaler class.Scaling can be done between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler, respectively.\n",
    "\n",
    "The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.394 0.837 0.295 0.628 0.496 0.    0.253 0.46  0.647 1.    0.432 0.758\n",
      " 0.207 0.14  0.185 0.488 0.592 0.036 0.364]\n"
     ]
    }
   ],
   "source": [
    "# Rescaling Burst Time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX = scaler.fit_transform(X[:,1].reshape(-1,1)) #picking column 1 for rescaling\n",
    "np.set_printoptions(precision=3)\n",
    "X[:,1] = rescaledX.reshape(1,-1)\n",
    "print(X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling (also known as data normalization) is the method used to standardize the range of features of data. Since, the range of values of data may vary widely, it becomes a necessary step in data preprocessing while using machine learning algorithms. Scaling is important in the algorithms such as support vector machines (SVM) and k-nearest neighbors (KNN) where distance between the data points is important. For example, in the dataset containing prices of products; without scaling, KNN might treat 1 USD equivalent to 1 PKR though 1 USD = 20 PKR. It is important to note that scaling does not change the data distribution. \n",
    "\n",
    "\n",
    "### Normalizing the Values of the Varaibles\n",
    "\n",
    "The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "Normal distribution (Gaussian distribution), also known as the bell curve, is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean.\n",
    "\n",
    "The preprocessing module provides a utility class Normalizer that implements the same operation using the Transformer API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 1.179e-03, 2.058e-03, 2.994e-03],\n",
       "       [1.000e+00, 3.579e-03, 3.333e-03, 0.000e+00],\n",
       "       [9.999e-01, 2.136e-03, 6.630e-03, 7.246e-03],\n",
       "       [1.000e+00, 1.357e-03, 1.037e-03, 0.000e+00],\n",
       "       [1.000e+00, 1.752e-03, 1.961e-03, 0.000e+00],\n",
       "       [1.000e+00, 0.000e+00, 7.528e-03, 0.000e+00],\n",
       "       [1.000e+00, 6.400e-04, 4.987e-04, 2.525e-03],\n",
       "       [1.000e+00, 9.788e-04, 2.101e-03, 2.128e-03],\n",
       "       [1.000e+00, 1.932e-03, 8.209e-05, 0.000e+00],\n",
       "       [1.000e+00, 3.676e-03, 7.904e-04, 2.042e-03],\n",
       "       [1.000e+00, 1.824e-03, 2.036e-03, 4.219e-03],\n",
       "       [1.000e+00, 2.382e-03, 1.785e-03, 3.145e-03],\n",
       "       [9.999e-01, 2.459e-03, 3.244e-03, 1.190e-02],\n",
       "       [1.000e+00, 4.518e-04, 1.544e-03, 0.000e+00],\n",
       "       [1.000e+00, 1.132e-03, 2.822e-03, 0.000e+00],\n",
       "       [1.000e+00, 1.076e-03, 1.711e-04, 2.207e-03],\n",
       "       [1.000e+00, 3.365e-03, 4.006e-03, 0.000e+00],\n",
       "       [1.000e+00, 7.976e-05, 5.679e-04, 2.227e-03],\n",
       "       [9.949e-01, 3.289e-02, 2.872e-02, 9.045e-02]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizing data\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "normalizedX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "\n",
    "In many Machine-learning or Data Science activities, the data set might contain text or categorical values (basically non-numerical values). For example, color feature having values like red, orange, blue, white etc. Meal plan having values like breakfast, lunch, snacks, dinner, tea etc. Few algorithms such as decision-trees can handle categorical values very well but most of the algorithms expect numerical values to achieve state-of-the-art results\n",
    "\n",
    "#### Label Encoding\n",
    "This approach is very simple and it involves converting each value in a column to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sunny' 'hot']\n",
      " ['sunny' 'hot']\n",
      " ['overcast' 'hot']\n",
      " ['rainy' 'mild']\n",
      " ['rainy' 'cool']\n",
      " ['rainy' 'cool']\n",
      " ['overcast' 'cool']\n",
      " ['sunny' 'mild']\n",
      " ['sunny' 'cool']\n",
      " ['rainy' 'mild']\n",
      " ['sunny' 'mild']\n",
      " ['overcast' 'mild']\n",
      " ['overcast' 'hot']\n",
      " ['rainy' 'mild']]\n",
      "['no' 'no' 'yes' 'yes' 'yes' 'no' 'yes' 'no' 'yes' 'yes' 'yes' 'yes' 'yes'\n",
      " 'no']\n"
     ]
    }
   ],
   "source": [
    "ds = pd.read_excel('weatherTemp.xlsx')\n",
    "# splitting the non-class and class attributes\n",
    "x = ds.iloc[:, 0:2].values  #The iloc indexer for Pandas Dataframe is used for \n",
    "y = ds.iloc[:, 2].values #integer-location based indexing / selection by position.\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather: [2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n",
      "Temp: [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "Play: [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n",
      "[[2 1]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "#Converting String Labels into numbers one column at a time\n",
    "x[:,0]=encoder.fit_transform(x[:,0])\n",
    "x[:,1] = encoder.fit_transform(x[:,1])\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "#Observing the transformed data set\n",
    "print(\"Weather:\", x[:,0])\n",
    "print(\"Temp:\", x[:,1])\n",
    "print(\"Play:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes and Gaussian Naive Bayes\n",
    "Classical Naive Bayes supports categorical features and models each as conforming to a Multinomial Distribution. Gaussian Naive Bayes supports continuous valued features and models each as conforming to a Gaussian (normal) distribution. The bottom line is that when your data set’s features are all categorical, the classical Naive Bayes classifier is appropriate. When you data set’s features are all continuous, the Gaussian Naive Bayes classifier is appropriate. If your data set contains both categorical and continuous features, you have two options. You can either discretize your continuous features via bucketing or a similar methodology, or you can use a hybrid Naive Bayes model. Unfortunately none of the standard machine learning libraries seem to include such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a Multinomail Naive Bayes Classifier\n",
    "model = MultinomialNB()\n",
    "\n",
    "#Train the model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: [1]\n"
     ]
    }
   ],
   "source": [
    "#predict Output\n",
    "predicted = model.predict([[0,1]]) #input coming as overcast and Mild\n",
    "print(\"Predicted Value:\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Algorithm\n",
    "Confusion matrix, precision, recall, and F1 measures are the most commonly used metrics for classification tasks. Scikit-Learn's metrics library contains the classification_report and confusion_matrix methods, which can be readily used to find out the values for these important metrics.\n",
    "\n",
    "Here is the code for finding these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [4 1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      1.00      0.33         1\n",
      "         1.0       1.00      0.20      0.33         5\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.60      0.60      0.33         6\n",
      "weighted avg       0.87      0.33      0.33         6\n",
      "\n",
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)\n",
    "result2 = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy:\",result2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
